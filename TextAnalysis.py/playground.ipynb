{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chan_io as io\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import nltk\n",
    "from tensorflow import keras\n",
    "from tensorflow import strings\n",
    "import tensorflow as tf\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Playground for testing analysis tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['./exampleThreads.json']\n",
    "paths = io.getFiles(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./exampleThreads.json']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.DataFrame(columns=[\"timestamp\",\"content\"], index=pd.Index([],dtype=int, name=\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseOp(jsonPost):\n",
    "    \"\"\"Convert from json original post to timestamp, text tuple\"\"\"\n",
    "    title = \"\" if jsonPost['title'] is None else jsonPost['title']\n",
    "    comment = \"\" if jsonPost['comment'] is None else jsonPost['comment']\n",
    "    return [int(jsonPost[\"timestamp\"]), \" \".join([title, comment])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseThread(base_id, jsonThread):\n",
    "    \"\"\"Convert from json to pandas dataframe with columns timestamp and content and index of ids\"\"\"\n",
    "    df = pd.DataFrame(columns=[\"timestamp\",\"content\"], index=pd.Index([],dtype=int, name=\"id\"))\n",
    "    id = base_id * 100\n",
    "    op = parseOp(jsonThread[\"op\"])\n",
    "    df.loc[id] = op\n",
    "    for p in jsonThread[\"replies\"]:\n",
    "        id += 1\n",
    "        df.loc[id] = [op[0],p]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in paths:\n",
    "    with open(p) as file:\n",
    "        try:\n",
    "            threads_json = json.load(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read json file for threads: {e}\")\n",
    "            sys.exit(-1)\n",
    "        \n",
    "        try:\n",
    "            for t in threads_json.items():\n",
    "                df_posts = df_posts.append(parseThread(int(t[0]), t[1]))\n",
    "        except Exception as e:\n",
    "            print(f\"Malformed json objects: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2485190600</th>\n",
       "      <td>1388521688</td>\n",
       "      <td>friendly reminder that making fun of dead naz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483806200</th>\n",
       "      <td>1388482949</td>\n",
       "      <td>What am I looking at?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483806201</th>\n",
       "      <td>1388482949</td>\n",
       "      <td>race war now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483806202</th>\n",
       "      <td>1388482949</td>\n",
       "      <td>&gt;Fights corruption using knowledge\\n\\nWhats th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483806203</th>\n",
       "      <td>1388482949</td>\n",
       "      <td>Don't get your knickers in a twist about this....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                                            content\n",
       "id                                                                       \n",
       "2485190600  1388521688   friendly reminder that making fun of dead naz...\n",
       "2483806200  1388482949                              What am I looking at?\n",
       "2483806201  1388482949                                       race war now\n",
       "2483806202  1388482949  >Fights corruption using knowledge\\n\\nWhats th...\n",
       "2483806203  1388482949  Don't get your knickers in a twist about this...."
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizer(text: str) -> str:\n",
    "    \"\"\"A method for standardizing a given string tensor\"\"\"\n",
    "    lowercased = text.lower()\n",
    "    html_stripped = strings.regex_replace(lowercased, \"<[^>]+>\", \" \")\n",
    "    punctuation_stripped = strings.regex_replace(html_stripped, \"[%s]\" % re.escape(string.punctuation), \"\").numpy().decode('utf-8')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = \" \".join(map(lemmatizer.lemmatize, word_tokenize(punctuation_stripped)))\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = df_posts.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.loc[:,\"content\"] = df_posts.loc[:,\"content\"].map(standardizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array([\"hate\",\"love\",\"joy\",\"happy\",\"anger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeLayer(vocab: np.ndarray) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Create a tensor for text vectorization based on the given vocabulary as a numpy array.\n",
    "    If multi_hot = False, count of vocab occurrences is ignored.\n",
    "    See the documentation on keras' TextVectorization layer for more\n",
    "    \"\"\"\n",
    "    return keras.layers.TextVectorization(\n",
    "        standardize=None,\n",
    "        output_mode='multi_hot',\n",
    "        vocabulary = vocab\n",
    "    )\n",
    "\n",
    "def mapTokenCounts(text: str, vocab: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    map the given post to matched words from the vocabulary\n",
    "    \"\"\"\n",
    "    vector_layer = vectorizeLayer(vocab)\n",
    "    t = tf.constant(text)\n",
    "    counts = vector_layer(t).numpy()[1:]\n",
    "    counts = counts.astype(np.bool8)\n",
    "    return \" \".join(vocab[counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.loc[:,\"matched_vocab\"] = df_posts.loc[:,\"content\"].map(lambda x: mapTokenCounts(x, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'happy', 'love', 'hate love', 'hate', 'hate happy', 'anger',\n",
       "       'hate love joy', 'hate love happy', 'love anger',\n",
       "       'hate love anger', 'hate joy', 'joy', 'love joy', 'happy anger',\n",
       "       'love happy'], dtype=object)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts.loc[:,\"matched_vocab\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 19:46:01.987383: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-17 19:46:01.987522: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = io.extractVocab(\"./hate_vocabulary.json\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7fa378472cf101e707a5468465ac7543fc3c373c0c5f6dc4778db26bbb1e212"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
